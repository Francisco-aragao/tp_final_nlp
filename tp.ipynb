{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "data = pd.read_csv('./data/anime-dataset-2023.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see column names\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get only the useful columns -> 'anime_id', 'Name', 'Genres', 'Synopsis'\n",
    "data = data[['anime_id', 'Name', 'Genres', 'Synopsis']]\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into 2 parts -> train and test\n",
    "# train -> 80% of the data\n",
    "# test -> 20% of the data\n",
    "train = data.sample(frac=0.8, random_state=0)\n",
    "test = data.drop(train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape, train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the genre of the anime\n",
    "\n",
    "# get the first anime synopsis\n",
    "anime_synopsis = test['Synopsis'].iloc[0]\n",
    "anime_name = test['Name'].iloc[0]\n",
    "anime_genre = test['Genres'].iloc[0]\n",
    "\n",
    "# all possible genres\n",
    "all_genres = list(set(data['Genres'].str.cat(sep='|').split('|')))\n",
    "\n",
    "print(all_genres)\n",
    "\n",
    "genres = set()\n",
    "for i in all_genres:\n",
    "    genres_separated = i.replace(' ', '').split(',')\n",
    "    for j in genres_separated:\n",
    "        genres.add(j)\n",
    "\n",
    "print(genres)\n",
    "\n",
    "genres = list(genres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(anime_synopsis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeroshot_classifier = pipeline(\"zero-shot-classification\", model=\"MoritzLaurer/deberta-v3-large-zeroshot-v2.0\")  # change the model identifier here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "# Initialize results dictionary\n",
    "results = {}\n",
    "\n",
    "# Iterate over the test data with progress bar\n",
    "for index, row in tqdm(test.iterrows(), total=test.shape[0], desc=\"Processing anime\"):\n",
    "    anime_id = row['anime_id']\n",
    "    anime_synopsis = row['Synopsis']\n",
    "    anime_name = row['Name']\n",
    "    anime_genre = row['Genres']\n",
    "\n",
    "    text = 'The anime is called ' + anime_name + '. The synopsis of the anime is: ' + anime_synopsis\n",
    "    output = zeroshot_classifier(text, list(genres), multi_label=True)\n",
    "\n",
    "    anime_genre_len = len(anime_genre.split(','))\n",
    "\n",
    "    top_k_labels_predicted = output['labels'][0:anime_genre_len]\n",
    "\n",
    "    results[anime_id] = {}\n",
    "\n",
    "    for i in range(anime_genre_len):\n",
    "        genre = anime_genre.split(',')[i].strip()\n",
    "        results[anime_id][genre] = \"error\"\n",
    "        if genre in top_k_labels_predicted:\n",
    "            results[anime_id][genre] = \"correct\"\n",
    "\n",
    "# Save the results to a JSON file\n",
    "with open('results_based_model.json', 'w') as f:\n",
    "    json.dump(results, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "def preprocess(df):\n",
    "    df['text'] = 'The synopsis of the anime is: ' + df['Synopsis']\n",
    "    # Ensure genres are properly encoded as binary vectors for multi-label classification\n",
    "    unique_genres = sorted(set(g for genre_list in df['Genres'] for g in genre_list.split(',')))\n",
    "    genre_to_id = {genre: idx for idx, genre in enumerate(unique_genres)}\n",
    "\n",
    "    def encode_labels(genres):\n",
    "        labels = [0] * len(unique_genres)\n",
    "        for genre in genres.split(','):\n",
    "            labels[genre_to_id[genre]] = 1\n",
    "        return labels\n",
    "\n",
    "    df['labels'] = df['Genres'].apply(encode_labels)\n",
    "    return df, unique_genres\n",
    "\n",
    "train, unique_genres = preprocess(train)\n",
    "test, _ = preprocess(test)\n",
    "\n",
    "# Convert to Hugging Face Dataset\n",
    "train_dataset = Dataset.from_pandas(train[['text', 'labels']])\n",
    "test_dataset = Dataset.from_pandas(test[['text', 'labels']])\n",
    "\n",
    "# Tokenizer\n",
    "model_name = \"MoritzLaurer/deberta-v3-large-zeroshot-v2.0\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize(batch):\n",
    "    tokenized = tokenizer(batch['text'], padding=\"max_length\", truncation=True, max_length=512)\n",
    "    tokenized[\"labels\"] = batch[\"labels\"]\n",
    "    return tokenized\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize, batched=True)\n",
    "\n",
    "# Load Pre-trained Model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(unique_genres),  # Set to the number of genres in your dataset\n",
    "    ignore_mismatched_sizes=True   # Ignore size mismatches\n",
    ")\n",
    "\n",
    "# Freeze pre-trained layers\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset\n",
    ")\n",
    "\n",
    "# Train\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
